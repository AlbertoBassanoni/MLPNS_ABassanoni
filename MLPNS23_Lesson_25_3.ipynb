{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuCX14qI7LtksiAX0A7HA9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlbertoBassanoni/MLPNS_ABassanoni/blob/main/MLPNS23_Lesson_25_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Likelihood and Optimization:\n",
        "\n",
        "La likelihood consiste nel ricavare la distribuzione di probabilità del modello noti i nostri i dati. Si danno delle assunzioni gaussiane:\n",
        "\n",
        "**$P(x)=\\frac{1}{\\sqrt{2\\pi\\sigma}}e^{\\frac{(x-\\mu)^2}{\\sigma^2}}$**\n",
        "\n",
        "Una tecnica per trovare qual'è la migliore probabilità e conseguentemente la mia likelihood si usa la gradient descent algorithm, in cui:\n",
        "\n",
        "**$p_{new}:=p_{old}-\\eta \\nabla Q(p)$**\n",
        "\n",
        "Il gradient descent algorithm funziona così:\n",
        "\n",
        "1) Scelgo una funzione target Q(p)\n",
        "\n",
        "2) Scelgo un valore iniziale per i parametri **p_i=(a_i,b_i)**;\n",
        "\n",
        "3) Scelgo un learning rate **$\\eta$**;\n",
        "\n",
        "4) Calcolo il gradiente di Q della target function per i valori correnti dei parametri, calcolandoli su tutte le osservabili del training set;\n",
        "\n",
        "5) Calcolo il nuovo stepsize per ogni feature, facendo **$stepsize=\\nabla Q \\eta$**;\n",
        "\n",
        "6) Setto la nuova probabilità **$p_{new}=p_{old} - stepsize$**;\n",
        "\n",
        "7) Ripeto 4), 5), 6) fino a convergenza;\n",
        "\n",
        "\n",
        "Problema di questo metodo! Potrei finire in un minimo locale, che potrebbe non essere il minimo globale del mio spazio dei parametri. Per risolvere il problema devo introdurro uno stochastic gradient descent algorithm, introducendo una stocasticità nel calcolo della direzione del gradiente. \n",
        "\n",
        "Lo stochastic gradient descent algorithm funziona così:\n",
        "\n",
        "1) Scelgo una funzione target Q(p)\n",
        "\n",
        "2) Scelgo un valore iniziale per i parametri **p_i=(a_i,b_i)**;\n",
        "\n",
        "3) Scelgo un learning rate **$\\eta$**;\n",
        "\n",
        "4) Calcolo il gradiente di Q della target function per i valori correnti dei parametri, calcolandoli su un subset casuale del training set;\n",
        "\n",
        "5) Calcolo il nuovo stepsize per ogni feature, facendo **$stepsize=\\nabla Q \\eta$**;\n",
        "\n",
        "6) Setto la nuova probabilità **$p_{new}=p_{old} - stepsize$**;\n",
        "\n",
        "7) Ripeto 4), 5), 6) fino a convergenza;\n",
        "\n",
        "La differenza sta nel punto 4)"
      ],
      "metadata": {
        "id": "tHVGylHeU2Ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MonteCarlo Markov Chains:\n",
        "\n",
        "I metodi MonteCarlo si riferisce storicamente alla città di MonteCarlo dove stavano i casinò. Storicamente nasce da Stanislav Ulav e con il gioco del solitario. Al cuore del MonteCarlo iniziavano a nascere i computers per fare delle simulazioni MonteCarlo (anche nel progetto Manhattan sono state molto rilevanti!)\n",
        "\n",
        "Ora vediamo come fare problemi di ottimizzazione con le MonteCarlo Markov Chains. MonteCarlo significa \"stocastico\", Markov si intende \"markoviano\" e Chain si intende \"sequenza\". Abbiamo a che fare con sequenze stocastiche markoviane. \n",
        "\n",
        "Il goal è di trovare i parametri per massimizzare **P(\\theta|D,f)** nel teorema di Bayes. A me interessa un posterior con la joint probability distribution di un set di parametri (m,b) condizionati da dei dati D sotto delle ipotesi del modello f. \n",
        "\n",
        "Calcolo delle marginalized probability P(m) e P(b), e poi plotto assieme tutto. Quello che stiamo cercando è quella di fare una completa analisi della likelihood surface, ossia della target function. Quando ho una full likelihood surface, l'istogramma mi consente non solo di dire qual'è il valore migliore, ma mi permette di calcolarne anche la varianza e la covarianza!\n",
        "\n",
        "Il MCMC si sviluppa così:\n",
        "\n",
        "Scelgo uno starting point nel parameter space, scegliendo **$\\theta_0=(m_0,b_0)$**. \n",
        "\n",
        "WHILE ... (metto un criterio di convergenza)\n",
        "\n",
        "scelgo un nuovo set di parametri\n",
        "\n",
        "calcolo la posterior probability\n",
        "\n",
        "if **$p_{new}/p_{err}>1$**\n",
        "\n",
        "current=new\n",
        "\n",
        "ELSE:\n",
        "\n",
        "scelgo un random number\n",
        "\n",
        "  if **$p_{new}/p_{err}>1$**\n",
        "\n",
        "  current=new\n",
        "\n",
        "  ELSE\n",
        "\n",
        "  pass// do nothing\n",
        "\n",
        "\n",
        "La stocasticità ci permette di esplorare l'intera superficie ma spende più tempo negli spots più interessanti. L'aspetto Markoviano è per il fatto che la posizione che scelgo nel passaggio successivo è indipendente dalle scelte precedenti. E' inoltre un processo ergodico, cioè reversibile. Un processo Markoviano ed ergodico in un tempo t sufficentemente lungo, è possibile esplorare l'intera superficie. \n",
        "\n",
        "Uno dei punti chiave sarà capire se dopo un certo tempo finito il processo è andato avanti abbastanza per trovare il global minimum/maximum che abbiamo trovato. Ci sono un paio di regole o criteri per stabilirlo. Ci sono diversi algoritmi per farlo. Quello che guardiamo è un modello che farà andare molte catene nello stesso tempo. Diamo un initial guess, rimangono vicine a quella proprietà per un po', la ricerca si espande e poi converge sul punto di ottimizzazione. Il risultato che ottengo sarà una media bidimensionale del plot sulle catene. \n",
        "\n",
        "La differenza chiave tra un algoritmo MCMC e l'altro è la scelta della regola di convergenza e del passing del mio valore random. Uno dei più famosi è il Metropolis algorithm, e quello che applichiamo oggi è l'EMCEE. L'idea è di avere molte catene in parallelo che esplorano la superficie in maniera indipendente. Ogni catena è mutualmente indipendente, e soddisfa la definizione di markovianità e di ergodicità. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VCWGmEypXv7t"
      }
    }
  ]
}