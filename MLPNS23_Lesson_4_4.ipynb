{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNC10qchUqIsSuXPojxkIhO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlbertoBassanoni/MLPNS_ABassanoni/blob/main/MLPNS23_Lesson_4_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extraction of Features:\n",
        "\n",
        "Se vogliamo utilizzare dei metodi di machine learning io devo scegliere:\n",
        "\n",
        "- Delle time series e standardizzarle (con media 0 e varianza 1);\n",
        "\n",
        "- Per ogni time stamp lo devo comparare con il valore d'aspettazione;\n",
        "\n",
        "Essenzialmente ogni datapoint deve essere trattato come una feature per potervi fare clustering. \n",
        "\n",
        "Se uso la raw data representation c'è un problema di dimensionalità, e se voglio avere una curva temporale \"ricca\", devo collezionare molti dati nel corso del tempo. Genericamente questo richiede che la feature space sia high dimensional, e se vogliamo studiare delle time series ad esempio sul consumo di energia elettrica, ci sono molte scale temporali su cui poter avere informazioni. Devo avere una risoluzione alta per le mie time series, e con un feature space con dimensionalità così alta, il problema pratico è che i modelli che utilizziamo scalano con un modello almeno lineare in funzione del numero **$N$** di variabili. Quindi per questo può essere conveniente non utilizzare direttamente la raw representation delle time series. Altro problema, è che le time series potrebbero essere asincrone, e può essere un problema nello studio della periodicità dei miei trends. Ciò richiede un preprocessing dispendioso e complicato. Invece, con una rappresentazione astratta della periodicità, il tutto è più facile. Un altro problema è che le time series possono essere di durata diversa. \n",
        "\n",
        "Conviene usare una low dimensional representation, in cui è più interessante estrarre delle proprietà statistiche descrittive, quali:\n",
        "\n",
        "- media\n",
        "- standard deviation\n",
        "- skewness\n",
        "- kurtosis\n",
        "\n",
        "E posso lavorare in questa descrizione statistica dei dati come feature space, e posso utilizzare dei modelli parametrici per descrivere dei dati. Ad esempio, se fisso una linea che fitta le mie time series, le slopes e le intercepts di questi fit diventano le nuove variabili del feature space che posso trattare con clustering. In questo caso è molto più semplice utilizzare algoritmi di machine learning. Questo processo si chiama feature extraction o feature engineering. \n",
        "\n",
        "Quando faccio feature engineering i miei desiderata sono:\n",
        "\n",
        "- preservare le similarità tra coppie;\n",
        "- bound inferiori per accelerare la ricerca di similarità;\n",
        "- permettere di utilizzare prefissi per la rappresentazioni per metodi di scaling;\n",
        "- Supportare una computazione efficiente;\n",
        "- Supportare una decomposizione sostenibile dei dati in matrici di similarità;\n"
      ],
      "metadata": {
        "id": "zjDcYPKevqBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Learning:\n",
        "\n",
        "I metodi di supervised learning si parla di modelli per la classificazione e la predizione, e in particolare è utile il concetto di similarità. La similarità può essere usata in congiunzione a modelli parametrici e non parametrici. Ha bisogno di labels, in certi casi di molti labels. E' dipendente dalla definizione di similarità. \n",
        "\n",
        "Come sappiamo, la differenza tra modelli di clustering e modelli di classifying è che noi vogliamo nei modelli di classifying costruire un modello predittivo per il mio feature space. \n",
        "\n",
        "Questi modelli tipicamente restituiscono una partizione del mio feature space. Un modello molto prototipico è il SVM (supported vector machine), che funziona creando un iperpiano (una curva in 2D) che separa in maniera ottimale le osservazioni in base alle loro proprietà. Un altro algoritmo di classificazione sono i tree methods, quali random forests, e si parla in questo caso di una flow chart, ossia un algoritmo iterativo che separa in continuazione le mie variabili. Per il fatto che faccio la scelta su un asse alla volta, io ho una precisione maggiore rispetto ad un SVM."
      ],
      "metadata": {
        "id": "QwqdiQCq0VoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K - Nearest Neighbours:\n",
        "\n",
        "Questo algoritmo di classificazione assegna una classe ai punti più vicini. Per K=1 mi fermo ai primi vicini, per K=2 mi fermo ai secondi vicini, per K=3 mi fermo ai terzi vicini, etc...\n",
        "\n",
        "Questo algoritmo si basa ancora una volta sul calcolo della distanza, e il principale hyperparameter di questo modello è il numero di primi vicini K. Questo modello è utile, perché se aggiungo un nuovo dato non devo ricalcolare nuovamente tutte le distanze della mia metrica, ma solo la sua distanza con gli altri punti e selezionare i più vicini. \n",
        "\n",
        "Questo modello è computazionalmente dispendioso all'inizio, il cui costo cresce come O(d*N^2), ma una volta che aggiungo n nuovi punti, la crescita computazionale è solo di O (d*n)\n",
        "\n",
        "## Lazy-Learner K-Nearest Neighbours:\n",
        "\n",
        "Calcola la distanza d da tutti gli oggetti conosciuti. Seleziona i K più vicini.\n",
        "\n",
        "Classificazione:\n",
        "Assegna il più comune tra i k nearest neighbours.\n",
        "\n",
        "Regression:\n",
        "Predice il valore medio dei k nearest neighbours.\n",
        "\n",
        "Una cosa positiva di questo modello è che per esso vale il seguente teorema:\n",
        "\n",
        "TEOREMA: Per **$n->\\infty$**, l'errore del modello 1 Nearest Neighbours non è più grande del doppio dell'errore del Bayes Optimation classifier, ossia:\n",
        "\n",
        "**$arg \\max_y \\sum_{h_i \\in H}P(y|h_i)P(h_i|D)$**\n",
        "\n",
        "inolte, è un modello non parametrico e che funziona molto bene con un large training test.\n",
        "\n",
        "Non è utile per gli outliers, non funziona bene se il training set è sparso ed è buono fino a che la distanza metrica è un buon indicatore per la classificazione delle mie variabili!"
      ],
      "metadata": {
        "id": "dEe3Ot-E26vc"
      }
    }
  ]
}