{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNma8AcpnXfZofQ0hyLm1O4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlbertoBassanoni/MLPNS_ABassanoni/blob/main/MLPNS23_Lesson_23_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ESERCIZIO: Intermission - Gamma Ray Bursts\n",
        "\n",
        "I gamma ray bursts sono esplosioni stellari tradotte come emissioni energetiche nel regime dei gamma ray di durata tipicativamente breve e sono delle emissioni di sorgente stocastica, e si pensa siano originate dal merging di stelle di neutroni, che dà luogo ad un'emissione collimata.\n",
        "\n",
        "Nei regimi ottici questa emissione si chiama afterglow. Utilizzeremo dei dati di gamma ray bursts in frequenze ottiche. Nei dati avremmo diverse bande di frequenza, e in astrofisica noi misuriamo la luminosità (lightcurve) che si misura in magnitudine.\n",
        "\n",
        "Le magnitudini sono una scala logaritmica della luminosità. Il motivo per cui si usa è storico, infatti la risposta dei nostri occhi alla luminosità è logaritmica, e ciò ci è utile in questo caso poiché il processo di energy decay segue una legge esponenziale del tipo **$10^x$**. Convertendolo in una scala logaritmica di tipo **$log_{10}(x)$**, ottenendo una legge lineare. La nostra legge è:\n",
        "\n",
        "**$m=25-2.5*log_{10}(flux)$**\n",
        "\n",
        "Quello che noi dovremmo fare è fare delle regressioni lineari dei vari dati a diversi range di frequenze. C'è un'area specifica della statistica per eliminare gli upper limits, e lo faremo in circa 8 modi diversi questo linear fitting. \n",
        "\n",
        "Nel logspace si può fare un linear fit, e come mai fittare linee ai dati? Vogliamo fare una predizione, pensando a time serie nel futuro. Fitto un modello lineare, e in base a quanto correttamente un modello fitta con una power law vedremo se la nostra predizione è falsificabile oppure no. \n",
        "\n"
      ],
      "metadata": {
        "id": "WIpMe86d8wbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression\n",
        "\n",
        "La linear regression si può risolvere analiticamente attraverso un po' di linear algebra, e si può fare la soluzione:\n",
        "\n",
        "**$(X^T * X)^{-1} * X^T * \\vec{y}= (a,b)$**\n",
        "\n",
        "L'independent variable è:\n",
        "\n",
        "**$X=(1 , x_1; 1 , x_2; ... ; 1 , x_m)$**\n",
        "\n",
        "Da sklearn si può fare la linear regression, al posto che fare la linear algebra. Quando utilizzo questi modelli, tipicamente utilizzo:\n",
        "\n",
        "lr.fit(X, y)\n",
        "\n",
        "Python è un liguaggio object oriented, e possiamo anche cleare classi di variabili che vuole modelli ed attributi. La linea di codice:\n",
        "\n",
        "lr = LinearRegression()\n",
        "\n",
        "Invoca una classe per creare un oggetto nella mia stack di memoria. Linear Regression è una classe con metodi ed attributi, e per fittare il modello in questa classe, applico il metodo \".fit\", e questo lo popola con altri attributi del modello. \n",
        "\n",
        "Se vado a vedere cosa sono i nuovi attributi:\n",
        "\n",
        "lr.coef_, lr.intercept_"
      ],
      "metadata": {
        "id": "inazvEJ0FqQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Objective Funcion\n",
        "\n",
        "Se non c'è una soluzione analitica, per selezionare il \"best\" set dei parametri. Qui abbiamo ignorato bellamente gli errori, e può accadere che il mio fit sia parzialmente corretto, e che sia buono a tempi iniziali rispetto a tempi lunghi. Se si integrano gli errori, il fit cambia drasticamente!\n",
        "\n",
        "L'objective function è alla base del Machine Learning, e per far sì che il computer \"impari\" i migliori valori bisogna dare un obbiettivo. L'objective function è l'elemento che ci permette di dire quali sono i risultati corretti e bisogna dare un reward system, ossia fare un'ottimizzazione che minimizzino o massimizzino i parametri. \n",
        "\n",
        "Per selezionare il best set di parametri ci serve un piano: dobbiamo scegliere la funzione che minimizzino o massimizzino:\n",
        "\n",
        "**$L_1=\\sum_i^N |f(x_i)-y|$**\n",
        "**$L_2=\\sum_i^N (f(x_i)-y)^2$**\n",
        "**$\\chi_2=\\frac{\\sum_i^N (f(x_i)-y)^2}{\\sigma_i^2}$**\n",
        "\n",
        "Con **$L_2$** do maggior peso agli errori più grandi, perché prendo la distanza squared, mentre **$L_1$** è biased perchè considera solo gli errori nel core della mia distribuzione. Nel **$\\chi^2$** si fa la stessa cosa del **$L_2$**, solo che si suppone che la distribuzione degli errori sulle varie ampiezze sia gaussiano con varianze **$\\sigma_i^2$**, e con distribuzione:\n",
        "\n",
        "**$P(x)= c*e^{\\frac{-(x-\\mu)^2}{\\sigma^2}}$**\n",
        "\n",
        "Ci sono molte opzioni per fare questo in Python, e ne troviamo nel pacchetto scipy.optimize. Ci sono diversi modi di ottimizzazione, e genericamente se usiamo come target function da minimizzare il chi^2, se scelgo come funzione:\n",
        "\n",
        "**$y_i=(a*x_i+b)$**\n",
        "\n",
        "Inseriamo nella target function:\n",
        "\n",
        "**$\\chi_2=\\frac{\\sum_i^N ((a*x_i+b)-y)^2}{\\sigma_i^2}$**\n",
        "\n",
        "Devo dargli dentro uno starting point initialGuess, e può avere bisogno dei dati.\n",
        "\n"
      ],
      "metadata": {
        "id": "oxmZYZNnPMrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validating a model\n",
        "\n",
        "Facciamo un train test fit, e misuriamo quando un modello è buono. Creo un subset di data per verificare se il modello è buono."
      ],
      "metadata": {
        "id": "z7iH7rZ4fEna"
      }
    }
  ]
}
