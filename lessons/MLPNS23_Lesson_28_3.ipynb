{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMG2OZrVk7ziXsP4IprBD6X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlbertoBassanoni/MLPNS_ABassanoni/blob/main/MLPNS23_Lesson_28_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Selection:\n",
        "\n",
        "Come scelgo qual'è un buon principio? Principio del rasoio di Occam: entia non sunt complicanda praeter necessitatem. Si sceglie la formula più semplice, con meno parametri. La domanda precisa da farsi è: è vero che il mio modello più complesso overfitta i dati?\n",
        "\n",
        "Esiste il test LR, che si aspetta che segua una chi^2 distribution sotto la null hypothesis che il modello più semplice è preferibile. \n",
        "\n",
        "La likelihood ratio statistics LR è:\n",
        "\n",
        "**$ LR=-2*ln(\\frac{L(complex model)}{L(simple model)}) $**\n",
        "\n",
        "Dove L(complex model) è la likelihood del modello più complesso e L(simple model) è la likelihood del modello più semplice.\n",
        "\n",
        "statsmodels.model.compare_lr_ratio() -> comando da applicare per provare il miglior modello.\n",
        "\n",
        "Questa comparazione si può fare anche tra modelli con diverso numero di parametri tra loro. Si può vedere tramite dei tabulati il confronto del P-value in questi casi. \n",
        "\n",
        "Ci sono altri tre modelli che vengono utilizzati nelle scienze fisiche:\n",
        "\n",
        "AIC -> Optimism and likelihood maximization on training set (probabilità frequentista)\n",
        "\n",
        "**$AIC= -2/N * log(L) + 2/N * k$**\n",
        "\n",
        "BIC -> based on bayesian probability\n",
        "\n",
        "**$BIC= -2 * log(L) + log(N) * k$**\n",
        "\n",
        "MDL -> based on definition of information entropy (bayesian probability)\n",
        "\n",
        "**$MDL= -log(L(\\theta)) - log(L(y|X,\\theta))$**\n",
        "\n"
      ],
      "metadata": {
        "id": "3z8cpKHF2IFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering:\n",
        "\n",
        "Parleremo di clustering. Ricapitoliamo che quando parliamo di ML parliamo di supervised e unsupervised learning. Nel caso di unsupervised learning vogliamo organizzare, compressare i dati, segnalare anomalie e compiere riduzioni dimensionali. Abbiamo già visto che il supervised learning ci permette di fare classificaioni e predizioni. La differenza chiave è che in supervised learning conosciamo il risultato su un subset dei dati, e vogliamo predirlo su diversi subset di dati. In unsupervised learning non abbiamo una conoscenza a priori di una variabile, ma vogliamo generare una nuova variabile legata ai dati, che ci connette al feature space. Quando parliamo di neural networks in letteratura parliamo di modelli supervised che si possono utilizzare per realizzare gli obbiettivi  dell'unsupervised learning. Il clustering è legato all'unsupervised learning.\n",
        "\n",
        "I machine learning models hanno rispettivamente:\n",
        "\n",
        "- parameters: modello ottimizzato dai dati (esempio: a e b in un fit lineare y=a*x+b, con y, x dati)\n",
        "\n",
        "- hyperparameters: parametri scelti dall'autore che possono essere costruiti in base al codice e alla mia domain knowledge (esempio: nwalkers nella MCMC)\n",
        "\n",
        "All'interno dei modelli più complessi avremmo più hyperparameters da scegliere. E' importante capire quali sono gli iperparametri più sensibili. \n",
        "\n"
      ],
      "metadata": {
        "id": "NdWS2mFL7ju7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduzione al Clustering\n",
        "\n",
        "Il clustering è un metodo di unsupervised learning. Tutte le features sono osservate per tutti gli oggetti del sample. Come dovrei raggruppare i dati nel feature space? Dipende dai miei obbiettivi! Ad esempio, se ho immagini di animali e sono uno zoologo, raggrupperò delle immagini in base alle loro famiglie naturali (ad esempio: uccelli, mammiferi, pesci, rettili...)\n",
        "\n",
        "Implicitamente, ho preso un:\n",
        "\n",
        "INTERNAL CRITERION: Membri del cluster devono essere simili l'un l'altro.\n",
        "\n",
        "EXTERNAL CRITERION: Membri esterni al cluster devono essere dissimili dagli oggetti all'interno dei dati.\n",
        "\n",
        "Ora però, se cambio mestiere e faccio il fotografo, potrei scegliere una diversa classificazione in base ai colori delle pelli degli animali! Quindi, per ogni algoritmo di clustering bisogna definire un criterio di scelta e una metrica che mi rappresenti la distanza tra due oggetti in base al criterio che ho scelto. \n",
        "\n",
        "Il clustering ottimale dipende da:\n",
        "\n",
        "- Come definire similarity/distance\n",
        "\n",
        "- L'obbiettivo del clustering\n",
        "\n",
        "I nostri algoritmi di clustering dovrebbero essere:\n",
        "\n",
        "- Scalabili (Attenzione: il clustering è un metodo molto costoso in termini computazionali! Non deve scalare troppo rapidamente col numero dei punti, basta non finire in un problema NP che scalano diversamente da una power law nei tempi di calcolo);\n",
        "\n",
        "- Ability to deal with different types of attributes;\n",
        "\n",
        "- Scoperta di clustering di ogni tipo e forma;\n",
        "\n",
        "- Minima conoscenza di domain knowledge (pochi hyperparameters);\n",
        "\n",
        "- Poter differenziare il noise e gli outliers (cioè supportare l'identificazioni di punti anomali);\n",
        "\n",
        "- Deve essere interpretabile;"
      ],
      "metadata": {
        "id": "jr6kokqN-uwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distance Metrics:\n",
        "\n",
        "Ci sono diverse formule per definire la distanza tra due punti. \n",
        "\n",
        "Nel caso di variabili continue, prendo solitamente la famiglia delle distanze di Minkowski. Dati N features:\n",
        "\n",
        "**$D(i,j)=\\sqrt[p]{\\sum_{k=1}^{N}|x_{ik}-x_{jk}|^p}$**\n",
        "\n",
        "Da questa serie di distanze ritrovo le mie loss functions. Infatti, per:\n",
        "\n",
        "- p=1 ottengo la Manhattan distance (L1)\n",
        "\n",
        "- p=2 ottengo la Euclidean distance (L2), nostra distanza preferita!\n",
        "\n",
        "In alcuni casi, come nel caso geospaziale, la Minkowski distance potrebbe non funzionare! Ad esempio, sulla sfera si può definire un'altra metrica. All'interno di scipy.spacial.distances ci sono encodate tantissime diverse distanze.\n",
        "\n",
        "Diventa molto più complicato definire una distanza attraverso delle variabili categoriche, ad esempio come definisco una distanza tra un falco e una tigre se faccio clustering? Potrei fare una combinazione di variabili numeriche e categoriche. Un esempio tipo di variabili categoriche sono quelle binarie, e se voglio definire la distanza tra due osservazioni in variabili binarie, posso misurare la co-presenza o la co-assenza di certi features nei miei dati. Posso definire ad esempio un simple matching coefficient:\n",
        "\n",
        "**$ SMC(i,j) =\\frac{M_{i=0,j=0}+M_{i=1,j=1}}{M_{i=0,j=0}+M_{i=1,j=0}+M_{i=0,j=1}+M_{i=1,j=1}}$**\n",
        "\n",
        "Ci sono altri modi di definire la distanza, come la Jaccard Similarity, utile per identificare il rapporto tra l'overlap e l'unione di due insiemi di dati A e B, ossia:\n",
        "\n",
        "**$J(i,j)=\\frac{A\\cap B}{A\\cup B}$**"
      ],
      "metadata": {
        "id": "oGadewXRAzul"
      }
    }
  ]
}
