{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3rSDtI2vQ+rVX0X7V6rRa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlbertoBassanoni/MLPNS_ABassanoni/blob/main/MLPNS23_Lesson_21_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ethics of Deep Neural Networks:\n",
        "\n",
        "Ogni modello lineare descritto tramite un one layer neural network fa un processo di back-propagation, ovverosia trova i migliori parametri trovando il minimo della loss function **$L_2$** nell'hyperplane **$(w_i, b)$**:\n",
        "\n",
        "**$L_2=\\sum_i (y_i^2-y_{i, pred})$** \n",
        "\n",
        "I training models con più layers e che contengono molti parametri richiedono cautela, ed è necessario calcolare la metrica della loss function, ed applico la correzione andando indietro, e bisogna fare attenzione. Trainare un deep neural network richiede un algoritmo di back-propagation. \n",
        "\n",
        "Genericamente, i deep neural networks non sono dei metodi particolarmente complicati o sofisticati, sono semplicemente linear models attivati da activation functions, con un gran numero di layer e di parametri per regressione, e ciò permette di imparare delle relazioni complicate. Un problema fondamentale nello studio ed applicazione dei neural networks è che la presenza di così tanti parametri li rendono molto difficili da interpretare. \n",
        "\n",
        "Ad esempio, dalla random forest si possono estrarre le features più importanti con relativa semplicità, mentre nel neural network no (black box problem). Questo potrebbe non essere un problema se mi interessa solo avere una predizione accurata. \n",
        "\n",
        "Quando si parla di etica dei dati ed epistemologia del data science, c'è il problema dell'epistemic transparency, ovverosia la conoscenza deve essere comunicabile e trasparente, e dipende dalle applicazioni che diamo ai modelli che creiamo. In alcuni casi, vi sono delle responsabilità etiche nella correttezza della risposta (ad esempio: algoritmi che riconoscono il viso di persone che compiono furti). Chi ha la responsabilità se l'algoritmo fa danni? (accountability) Ma anche in un ruolo sociale, ad esempio in campo medico o in campo finanziario per accettare prestiti, si deve avere right of explanation, ossia bisogna sempre dare una spiegazione per ogni decisione dell'intelligenza artificial. Altri focus etici sono:\n",
        "\n",
        "- Accountability\n",
        "\n",
        "- Value Alignment\n",
        "\n",
        "- Explainability\n",
        "\n",
        "- Fairness\n",
        "\n",
        "- User Data Rights\n",
        "\n",
        "Un grafico molto interessante è quello di una conferenza di Darpa (Ministero della Difesa USA) sul machine learning in cui si confronta l'accuratezza dell'algoritmo con l'intuitività del modello. Cioé accuracy vs explainability, e si ha che i modelli più accurati sono quelli più complessi e meno spiegabili, quali i neural networks in deep learning, che hanno un grandissimo feature space. Alla fine, torniamo sempre al rasoio di Occam: più il modello è semplice, meno parametri ha!  Un esempio particolarmente interessante sull'accountability è che alcuni sismologi italiani sono stati messi in galera per 6 anni poiché non hanno predetto un terremoto. Altro esempio sull'accountability è un uomo, Robert Williams, è stato arrestato ingiustamente poiché riconosciuto dall'intelligenza artificiale come un criminale. "
      ],
      "metadata": {
        "id": "GlAGLfgoRpN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Dreams:\n",
        "\n",
        "Utilizziamo un modello precostituito con un convolutial neural network. Ho la struttura bidimensionale di molti layers, ma il processo che viene fatto all'interno di ciascun neurone è spiegato al funzionamento del cervello. Come faccio a riconoscere un cane? Vedo feature visuali nel mio campo visivo, e so che le feature caratteristiche di un cane devono essere simili. Per farlo in un neural network, ciascuno dei neuroni diventa una forma specifica che va a cercare una zona dell'immagine in cui vi è più similarità. \n",
        "\n",
        "La cosa che qui inizializzo random e poi viene imparato dal neural network è la forma. Il neural network impara qual'è la miglior mappa che può essere poi utilizzata ad esempio per una classificazione. \n",
        "\n",
        "Il Deep Dream è un neural network già pretrainato con immagini su internet, e ciò ci consente di esplorare gli hidden layers del neural networks, cioé cosa vede il network nei layers intermedi? Ma siccome le immagini sono state prodotte su dataset precisi, qual'è l'obbiettivo di questi oggetti? Trovare oggetti o persone. Se spacchettiamo il neural network e guardiamo i layer intermedi, vediamo delle amplificazioni delle immagini sufficientemente similari alle immagini finali. Attenzione! Il train è stato ottimizzato per trovare animali o persone, e quindi troveremo dei layers intermedi con animali od oggetti ghost. \n",
        "\n"
      ],
      "metadata": {
        "id": "NiaH1SnuaKH4"
      }
    }
  ]
}
