{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMA7sfjiPN2ch1JNRGKjsm6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlbertoBassanoni/MLPNS_ABassanoni/blob/main/lessons/MLPNS23_Lesson_4_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model Performance:\n",
        "\n",
        "E' importante saper predirre l'accuratezza di un modello di Machine Learning, e abbiamo già visto che ad esempio nella regressione abbiamo utilizzato parametri di valutazione quali l'accuracy, recall e precision. In caso di predizione binaria, è molto facile e si può fare mediante la null hypothesis rejection. \n",
        "\n",
        "**$LR=\\frac{FalseNegative}{TrueNegative}$**\n",
        "\n",
        "Quella che costruiamo è una sorta di \"Confusion Matrix\" in cui abbiamo \n",
        "\n",
        "- H0 falsified / H0 True -> Important message spammed;\n",
        "- H0 not falsified / H0 True -> True negative;\n",
        "- H0 falsified / H0 False -> True positive:\n",
        "- H0 not falsified / H0 False -> Spam in your inbox;\n",
        "\n",
        "Ci sono tre possibili tecniche di misura di binary classification:\n",
        "\n",
        "**$Precision = \\frac{TP}{TP+FP}$**\n",
        "\n",
        "**$Recall = \\frac{TP}{TP+FN}$**\n",
        "\n",
        "**$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$**\n",
        "\n",
        "Il problema diventa più complicato qualora io abbia più di due classi, e in quel caso si utilizza come metrica per la performance di un modello l'F score, che non è nient'altro che la media armonica della precision e della recall. La funzione è:\n",
        "\n",
        "**$F_{\\beta} = (1+\\beta^2)\\frac{Precision * Recall}{(1+\\beta^2)TP + \\beta^2 FN + FP}$**\n",
        "\n",
        "Con **$\\beta$** fattore reale. E altro elemento importaante è il receiver operating characteristic, ossia una ROC curve, che misura il true positive rate vs il false negative rate. "
      ],
      "metadata": {
        "id": "DPn3oM24B1NE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT3 and Society:\n",
        "\n",
        "Vediamo quali sono alcune controversie sociali nel mondo dell'AI legato ai modelli di transformers, ovverosia dei modelli che fanno uso dell'attenzione per fare previsioni sulle time series analysis. Una delle applicazioni più recenti di questi modelli è la predizione del linguaggio, ossia creare dei chat box che danno risposte intelligenti se interrogati. Ci sono diverse conseguenze inaspettate degli NLP models, ad esempio si è evidenziato che con l'utilizzo dei transformers questi vengono trainati su selected internet contents, e il linguaggio impara le strutture sintattiche e il contenuto su internet. Il che crea fenomeni molto strani, ad esempio il contenuto può essere scorretto o pericolo (talvolta anche razzista!), e si va incontro a delle risposte molto biased. Nell'utilizzo della language translaction il sesso del soggetto viene attribuito sulla base del contenuto, e se per caso la lingua non prevede un gender nella struttura sintattica il modello biased lo traduce lo stesso, e se lo inventa! Un altro problema è che non si sa se il contenuto che si legge viene da un ChatBox o da altre fonti. \n",
        "\n",
        "C'è un paper molto importante che solleva delle interessanti questioni etiche sui transformers \"On the Dangers of Stochastic Parrots: Can Language Models be too big?\" (da leggere per l'esame, or maybe the other paper). Ci sono dei rischi nell'uso di questi modelli, tra cui:\n",
        "\n",
        "- Environmental costes;\n",
        "\n",
        "- Financial costs;\n",
        "\n",
        "- Opportunity cost;\n",
        "\n",
        "- Risk of substantial harms, including stereotyping, denigration, increasing in extremist ideology and wrongful arrest;"
      ],
      "metadata": {
        "id": "B2D26MGsGQEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanishing Gradient:\n",
        "\n",
        "Altri modelli per il time series analysis includono modelli di Deep Learning, e il grosso problema che hanno tutti questi modelli è il vanishing gradient.\n",
        "\n",
        "Abbiamo visto l'architettura del feed forward neural network:\n",
        "\n",
        "Input layer --> hidden layers --> output layer\n",
        "\n",
        "Questi NN imparano diverse informazioni sulle features, ma non imparano l'ordine con cui le varie features vengono espresse, e ciò è un problema se usiamo time series analysis. Allora si pensa di ricorrere ad un recurrent neural network:\n",
        "\n",
        "Input layer --> RNN hidden layers (loops) --> output layer\n",
        "\n",
        "Fondamentalmente usiamo una relazione di ricorrenza per ogni neurone del tipo **$h_t=f_q(h_{t-1},x_t)$**, dove **$h_t$** è il current state al tempo **$t$**, mentre **$x(t)$** è l'output layer. Diventa difficile valutare i time stants precedenti via via che il numero di connessioni dei neuroni cresce, e questo per via della backpropagation, ovverosia facciamo una catena di derivate che divengono sempre più piccole. Nel fare backpropagation successiva perdiamo sempre più informazione, e i layer più vicini imparano rapidamente, mentre quelli più lontani imparano più lentamente. Concettualmente, nelle strutture ricorrenti più la mia time series è lunga, più perdo informazione sugli elementi della time series precedente. Questo ci spiega perché questi modelli di Deep Learning perdono il contesto molto facilmente, e ciò crea tutti quei problemi tecnici ed etici che abbiamo visto. Ci sono alcune tecniche per risolvere questo problema, quale la long short term memory, introducendo dei gates all'interno dei layers di neuroni. "
      ],
      "metadata": {
        "id": "FEmYKDvPJbAk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention:\n",
        "\n",
        "La soluzione a questo problema è l'attenzione, che altro non è che una matrice di covariance tra l'input e l'output. Il concetto di attenzione è fondamentale per capire i transformers. I transformer models sono dei modelli aventi seguente architettura:\n",
        "\n",
        "- Encoder + Decoder architecture\n",
        "\n",
        "- Attention mechanism\n",
        "\n",
        "- Multiheaded attention\n",
        "\n",
        "Il meccanismo di attenzione è una covariance matrix che connette relazioni tra elementi della time series l'uno con l'altro. Ad esempio, nelle frasi si \"the cat that ake was fully and happy\" mette in correlazione le varie parole utilizzando dei weights per indicare la correlazione tra le varie parole, ad esempio \"cat\", \"fully\" e \"happy\" hanno alta correlazione. Ad ogni parola esistono dizionari specifici, e ad ogni parola associano un vettore di numeri, e si fa un'operazione di \"vector embedding\", e tra vettori posso calcolare la similitudine, ad esempio con il dot product. Ad esempio, se delle parole sono molto associate hanno un dot product alto e sono molto parallele, mentre parole poco associate hanno un dot product basso e sono quasi perpendicolari. I dati che vengono forniti ai transformers sono tokenized vectors, ed ora posso rifare il calcolo della matrice di covariance. \n",
        "\n",
        "Una delle innovazioni del transformers è che i meccanismi di attenzione sono multiheaded, cioè restituiscono più matrici di attenzione da cui il NN può imparare meglio relazioni complesse.\n",
        "\n",
        "L'attenzione prende in considerazione tre elementi importanti, che sono rispettivamente la key **$K$**, la value **$V$** e la query **$Q$**, termini che vengono dai retrieval systems. La definizione é:\n",
        "\n",
        "**$Attention(Q,K,V)=softmax( \\frac{QK^T}{\\sqrt{d_K}} ) V$**\n",
        "\n",
        "Il dot product tra **$Q$** e **$K^T$** agisce come uno scaling factor. Cosa sono? Dipende dal tipo di modello. A partire dall'encoder, io encodo l'input, e ho un processo di meccanicsmo di attenzione e un 8 layer perceptron. A seconda dei blocchi del transformer **$Q$**, **$T$** e **$V$** assumono significati differenti. \n",
        "\n",
        "C'è comunque all'interno dei transformers un encoder + decoder architecture che decodifica il passato e predice il futuro. Nel nostro modello in laboratorio facciamo classificazione di time series attraverso l'encoder di un transformer. "
      ],
      "metadata": {
        "id": "O3Dlqp5-Lo-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3gLdvqRZRvcw"
      }
    }
  ]
}