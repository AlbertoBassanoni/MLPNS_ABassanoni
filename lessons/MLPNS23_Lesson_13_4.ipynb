{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHwiDyUG9gl8EkRqPvzjCx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlbertoBassanoni/MLPNS_ABassanoni/blob/main/MLPNS23_Lesson_13_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tree Methods:\n",
        "\n",
        "Un modello tipico di supervised ML è la classification, in cui l'obbiettivo è classificare con una serie di labels dei subsets di variabili. Ad esempio il colore dei punti di un plot. Un metodo possibile che abbiamo visto è l'SVM, un metodo che utilizza un iperpiano che separa in maniera ottimale le variabili, e funziona bene quando si definisce una funzione polinomiale che crea i contorni dell'iperpiano (in 2D è una linea). Per alcuni tipi di variabili, come quelle categoriche, non è così utile. Il K Nearest Neighbors è più utile in quanto non separa lo spazio delle features, ma assegna delle classi ai neighbours più vicini (a distanza minore). E' un algoritmo che si utilizza anche per la scelta dei film preferiti su Netflix. \n",
        "\n",
        "Un algoritmo che vediamo oggi sono i tree methods, ovverosia algoritmi che splittano gli spazi lungo gli assi separatamente. E' tipicamente una flow chart, ossia una serie di selezioni binarie in cui il risultato della selezione precedente guida quella successiva. Ogni variabile nello spazio è considerata singolarmente. \n",
        "\n",
        "Ad esempio se nel mio spazio delle features il mio obiettivo della ricerca è il colore, faccio prima una ricerca sulla base di x, e poi sulla base di y:\n",
        "\n",
        "if x <= a:\n",
        "\n",
        "\tif y <= b:\n",
        "\n",
        "      \t\treturn blue\n",
        "\n",
        "return orange\n",
        "\n",
        "Gli algoritmi che si usano per individuare delle particelle in fisica delle alte energie sono dei gradient boosted trees, ed offrono una facilità notevole di implementazione, poiché permettono di classificare in base al peso, al flavour e al colore di una particella cosa abbiamo a che fare."
      ],
      "metadata": {
        "id": "KGNpUOjd2-EV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification and Regression Trees\n",
        "\n",
        "## Single Tree:\n",
        "\n",
        "Non utilizzero mai un singolo albero, poiché nella realtà il pacchetto di SKlearn usa un ensemble di single trees. L'esempio che facciamo è predirre la sopravvivenza dei passeggeri del Titanic in base alle caratteristiche del passeggero. Utilizzeremo i dati della Kaggle challenge per riprodurre questo modello. i dati distribuiti sono:\n",
        "\n",
        "- genere (uomo o donna, codificata come variabile categorica binaria)\n",
        "- classe del ticket (prima, seconda, terza classe, codificata come una variabile categorica)\n",
        "- età (codificata come una continous variable)\n",
        "\n",
        "Inizialmente ho 714 passeggero, con 424=Ns survivors e Nd=290 numero di morti. Si inizia dividendo il numero di Ns e Nd in male and female. Sono morti più passeggeri di gender male che gender female. Posso calcolare la probabilità di morte tra un passeggero maschio e un passeggero femmina, definendo una purità come:\n",
        "\n",
        "**$p=\\frac{N_{largest class}}{N_{total set}}$**\n",
        "\n",
        "In questo caso:\n",
        "\n",
        "**$M p=79%$**\n",
        "**$F p=75%$**\n",
        "\n",
        "Scegliendo come variabile categorica la classe del passeggero, ho la purity pari a:\n",
        "\n",
        "**$1° p=66%$**\n",
        "**$2°,3° p=54%$**\n",
        "\n",
        "Invece scegliendo l'age con una threshold di 6.5 anni, ho che per:\n",
        "\n",
        "**$<6.5 p=66%$**\n",
        "**$>6.5 p=61%$**\n",
        "\n",
        "Il modello migliore è chiaramente quello con maggior purità. Una tecnica che posso fare è suddividere più volte le mie variabili. Ad esempio prima sull'età, poi sul ticket class, e poi sull'age. Combinando un po' di scelte, possiamo scegliere quali siano i nodi con maggior purità, combinando un po' le possibili scelte fino a che non arrivo a una purity **p** soddisfacente. Può diventare complicato nel caso in cui io abbia delle variabili continue, perché come scelgo la threshold di una variabile continua? Ne ho infinite, e quindi dovrei avere un infinito tempo per esplorarle tutte!\n",
        "\n",
        "Il problema degli algoritmi ad albero è proprio questo: non posso fare un'esplorazione esaustiva delle threshold di tutte le variabili, specie se tutte continue! Quindi il problema è questo: differenti alberi conducono a diversi risultati! Devo dunque introdurre della randomness nel mio algoritmo per tenere conto di questo fattore. Per questo motivo non si usa mai un singolo albero. Cosa si usa in genere? Si usano due tipi di modelli:\n",
        "\n",
        "RANDOM FORESTS\n",
        "\n",
        "GRADIENT BOOSTED TREES\n",
        "\n",
        "Un albero è un hierarchical clustering model, in cui c'è una radice e dei nodi con delle branche successive. Ogni decisione binaria si chiama node, quello iniziale si chiama root node, e il raggruppamento con gli splitting dei nodes si chiama branch, mentre i nodes finali sono le leafs. \n",
        "\n",
        "Quali sono gli hyperparameters tipici di un tree? Quali variabili utilizzare, e quali sono le threshold? Ci sono:\n",
        "\n",
        "- criterio, nel nostro caso assunto come la purità, ma non è l'unica, la scelta più comune è il Gini criteria, oppure l'information entropy di Shannon;\n",
        "\n",
        "- max_depth, cioè numero massimo di ramificazioni dei nostri trees. Perché mai dover dare un massimo? Per evitare overfitting, e far sì che rientri nel nostro fitting anche il noise e non la fisica interessante del sistema. Esistono tecniche per la classificazione dell'overfitting (tipicamente lo fisso su valori bassi, tipo 5, 7 etc... Lo setto a valori bassi solo per feature spaces ad altissima dimensionalità);\n",
        "\n",
        "\n",
        "## Trees and Regression:\n",
        "\n",
        "Se voglio fare regression con i trees bisogna stare attenti a settare basso il max_depth, altrimenti rischio di fare overfitting e di fittare anche gli outliers dovuti al rumore. Nell'esempio in figura con max_depth=2 ho una curva più buona rispetto a quella con max_depth=5. "
      ],
      "metadata": {
        "id": "aB9Qbn-I8Qse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble Methods:\n",
        "\n",
        "I greedy models sono dei modelli in cui localmente cerco di ottimizzare i miei parametri. Di per sé, il singolo albero è un modello greedy, perché localmente ottimizza i parametri. \n",
        "\n",
        "La funzione che utilizzeremo è:\n",
        "\n",
        "sklearn.tree.DecisionTreeClassifier\n",
        "\n",
        "Questo modello viene fatto runnare N volte, e si utilizza un risultato collettivo da questi alberi. E' molto comune utilizzare degli ensemble methods, cioè faccio andare diverse versioni dello stesso modello con piccoli cambiamenti, che possono essere di natura progressiva o stocastica (ad esempio assegno casualmente al mio albero un subset di dati), ed estraggo dei risultati collettivi con cui aggregare le decisioni. \n",
        "\n",
        "Ci sono in generale due possibili ensemble methods:\n",
        "\n",
        "RANDOM FOREST:\n",
        "\n",
        "- Fa runnare degli alberi che vanno in parallelo;\n",
        "\n",
        "- Ogni albero usa un subset random di osservazioni/features (nome della randomizzazione: bootstraps or bagging);\n",
        "\n",
        "- Per la classificazione si decide per maggioranza, e si può utilizzare un metodo probabilistico per l'estrazione dei dati che mi interessano;\n",
        "\n",
        "- Si usa il comando sklearn.ensemble.RandomForestClassifier\n",
        "\n",
        "GRADIENT BOOSTED TREES:\n",
        "\n",
        "- Fa runnare degli alberi che vanno in serie;\n",
        "\n",
        "- Ogni albero usa dei pesi diversi per i features imparando i pesi dall'albero precedente (cioé è progressivo);\n",
        "\n",
        "- L'ultimo albero contiene la predizione per la classificazione;\n",
        "\n",
        "- Si usa il comando sklearn.ensemble.GradientBoostingClassifier \n",
        "\n",
        "\n",
        "Tipicamente i gradient boosted trees sono gli algoritmi migliori per i problemi di classificazione. Si possono utilizzare anche come regressors.  "
      ],
      "metadata": {
        "id": "XkYVD834Cgt1"
      }
    }
  ]
}
