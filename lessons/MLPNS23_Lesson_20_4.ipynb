{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjgmVyo83jAtgPKwXGTi6b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlbertoBassanoni/MLPNS_ABassanoni/blob/main/MLPNS23_Lesson_20_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Learning Introduction:\n",
        "\n",
        "In generale il concetto di intelligenza artificiale è un subset del machine learning, che è a sua volta un subset del deep learning. Oggi è più in voga il deep learning, e come al solito si usa per classificare in base ad esempi, capire la struttura del feature space, per fare regression e per capire quali features sono importanti nella predizione. \n",
        "\n",
        "Per i modelli di learning devo sempre definire delle loss functions, ossia i parametri del modello imparati vengono valutati come buoni o meno a seconda del valore della mia loss function. Abbiamo visto che per:\n",
        "\n",
        "- Unsupervised learning -> distance between points in clustering;\n",
        "\n",
        "- Supervised learning -> distance between real value and predicted value;\n",
        "\n",
        "## Neural Networks:\n",
        "\n",
        "Diamo una piccola overview storica dei neural networks, ed abbiamo l'inizio nel 1943 con il lavoro di McCulloch e Pitts in cui compare per la prima volta l'M-P neuron. Questo è un articolo di uno psicologo su come si comporta il cervello umano. I due hanno identificato che i neuroni celebrali funzionano con un meccanismo di \"all-or-none\", e si basa sulla propositional boolean logic \"true\" o \"false\". Inoltre, la struttura del cervello non cambia nel tempo. \n",
        "\n",
        "Dal punto di vista fisiologico, i neuroni sono composti da diversi elementi: i dendriti, ossia le parti che raccolgono gli input elettrici, il soma che fa passare tramite l'assone il segnale alle sinapsi, e le sinapsi diffondono il segnale ad altri dendriti. In un neural network abbiamo input datas che lavorano in un soma e dalle sinapsi fuoriesce un output state binario di tipo booleano. \n",
        "\n",
        "Qual'è la funzione matematica che rappresentiamo in questo modo?\n",
        "\n",
        "1 if **$\\sum_{i=1}^3 x_i \\geq \\theta$**\n",
        "\n",
        "0 if **$\\sum_{i=1}^3 x_i < \\theta$**\n",
        "\n",
        "Se **$x_i$** è Booleano (true/false), quale valore corrisponde alla scelta del parametro corrisponde all'operatore AND? **$\\theta=3$**. Se voglio l'operatore OR? **$\\theta=1$**.\n",
        "\n",
        "Un modello più sofisticato di neural network è il perceptron nel 1958 ad opera di F. Rosenblatt, ovverosia si costruisce un peso e una bias parameter. Con **$w_i$** pesi e un altro parametro **$b$** chiamato bias, che è fondamentalmente la struttura di un linear classifier. Si introduce un activation function come:\n",
        "\n",
        "**$y=f(\\sum_{i} w_i x_i + b)$**\n",
        "\n",
        "L'activation function è:\n",
        "\n",
        "y=1 if **$\\sum_{i=1}^3 w_i x_i + b \\geq Z$**\n",
        "\n",
        "y=0 if **$\\sum_{i=1}^3 w_i x_i +b < Z$**\n",
        "\n",
        "Come step function spesso si prende una sigmoid del tipo:\n",
        "\n",
        "**$f=\\sigma=\\frac{1}{1+e^{-z}}$**\n",
        "\n",
        "Ci sono tipiche activation functions, quali sigmoid, tanh, maxout, leaky reLU... e non si usa la step function classica ha il problema che la sua derivata non è continua! Ciò crea problemi se facciamo algoritmi di stochastic gradient descent. Vedremo che ci sono dei criteri per scegliere l'activation function in base al tipo di neuroni utilizzati, il tipo di metodo di learning (supervised o unsupervised), e il tipo di loss function. Il perceptron è quindi un neural network che si basa sulla multivariate linear regression, e l'unico hyperparameter è la activation function. Questo è uno dei primi veri algoritmi di intelligenza artificiale, e i parametri **$w_i$** e **$b$** sono i parametri imparati dal modello. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qsqfwkzF5iWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Neural Networks:\n",
        "\n",
        "Ora, invece che avere soltanto una funzione di attivazione, e quindi un solo layer che dia un peso ottimizzato, passiamo a una struttura a multi-layers del mio network, e ciò mi insegna che ci possono essere delle relazioni non lineari e non banali tra input ed output. Parliamo quindi un multilayer perceptron, in cui aggiungiamo degli hidden layers!\n",
        "\n",
        "Il multilayer perceptron è un grafo fully connected, in cui tutti i neuroni del layer precedente è connesso da un link a un neurone del layer successivo. Spesso non è necessario avere dei neural networks fully connected. Per ogni link nei miei layers passo la variabile in input con un peso associato, ad esempio se ho un input con **$x_1,x_2,x_3$** e un hidden layer con **$b_1,b_2,b_3,b_4$** bias, avrò rispettivamente tre set di 4 pesi:\n",
        "\n",
        "**$(w_{11},w_{12},w_{13},w_{14}),(w_{21},w_{22},w_{23},w_{24}),(w_{31},w_{32},w_{33},w_{34})$**\n",
        "\n",
        "Che vanno poi a finire in un altro layer avente **$w_1,w_2,w_3,w_4$** e un ultimo bias **$b$**. \n",
        "\n",
        "Vediamo ovviamente che questo tipo di network da un enorme problema di overfitting perché il numero di parametri da imparare è spaventosamente alto e cresce facilmente al crescere degli hidden layers. Tuttavia, il vantaggio grosso dei neural networks è quello di imparare delle relazioni di non linearità! Per evitare il rischio di overfitting, si deve passare al neural network una quantità enorme di dati (esempio: foto di cani, gatti etc...). La complessità è necessaria per potere imparare relazioni non lineari, e mitigo gli effetti di overfitting con un gran numero di dati in input.\n",
        "\n",
        "Ogni volta che devo fare un neural network devo sempre controllare output e loss function!! \n"
      ],
      "metadata": {
        "id": "v40nYa4QHUG8"
      }
    }
  ]
}
